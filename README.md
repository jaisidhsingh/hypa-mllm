# Training MLLM with Hyper-Alignment

## Notes

Llama-3.2-1B and Qwen-2.5-1.5B loaded together take 14GB of VRAM.

## Todos

- [x] Set up repo
- [x] Set up MLLM modelling
- [x] Write `src/model/model_tests.py`
- [x] Test `src/model/model_tests.py`
- [ ] Test `data/alignment_datasets.py`
- [ ] Write `src/training/trainers.py`
- [ ] Test `src/training/trainers.py`
- [ ] Write `src/training/train_mllm.py`
- [ ] Test `src/training/train_mllm.py`
- [ ] Resolve all bugs
